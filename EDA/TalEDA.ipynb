{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA TAL\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = pd.read_csv('../data/survey_results_20250205_154644.csv')\n",
    "users_df = pd.read_csv('../data/users_data_20250205_154700.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert strings into their appropriate type\n",
    "\n",
    "AKA convert strings that look like dicts/arrays to actual dicts and arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_convert(entry):\n",
    "    # Replace ObjectId with the string version of the ID\n",
    "    cleaned_entry = re.sub(r\"ObjectId\\('(.*?)'\\)\", r\"'\\1'\", entry)\n",
    "    # Convert the string to Python object\n",
    "    return ast.literal_eval(cleaned_entry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename email column to userId for consistency between DFs and drop the _id column since it's wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the corresponding columns for the user df\n",
    "users_df['form'] = users_df['form'].apply(clean_and_convert)\n",
    "\n",
    "final_users_df = pd.DataFrame()\n",
    "final_users_df['userId'] = users_df['email']\n",
    "\n",
    "final_users_df = pd.concat([final_users_df[['userId']], users_df['form'].apply(pd.Series)], axis=1)\n",
    "final_users_df = final_users_df.drop(columns=['_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey DF\n",
    "\n",
    "### Convert the appropriate columns from strings to their respective data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the columns for the survey df\n",
    "survey_df['windowDimensions'] = survey_df['windowDimensions'].apply(clean_and_convert)\n",
    "survey_df['gaze'] = survey_df['gaze'].apply(clean_and_convert)\n",
    "survey_df['formData'] = survey_df['formData'].apply(clean_and_convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the formData and windowDimensions columns in the survey df from a dict to being their own individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_survey_df = pd.concat([survey_df, survey_df['formData'].apply(pd.Series), survey_df['windowDimensions'].apply(pd.Series)], axis=1)\n",
    "final_survey_df = final_survey_df.drop(columns=['formData', '__v', '_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop extra start times if the duration exceeds 15 seconds since the videos are only 15 seconds long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(final_survey_df.shape[0]):\n",
    "    if len(final_survey_df['gaze'].iloc[i]) > 0:\n",
    "        endTime = final_survey_df['gaze'].iloc[i][-1]['time']\n",
    "        startTime = final_survey_df['gaze'].iloc[i][0]['time']\n",
    "\n",
    "        endTime = pd.to_datetime(endTime / 1000, unit='s')\n",
    "        startTime = pd.to_datetime(startTime / 1000, unit='s')\n",
    "        \n",
    "        duration = (endTime - startTime).total_seconds()\n",
    "\n",
    "        while len(final_survey_df['gaze'].iloc[i]) > 0 and duration > 15:\n",
    "            final_survey_df['gaze'].iloc[i].pop(0)\n",
    "\n",
    "            if len(final_survey_df['gaze'].iloc[i]) > 0:\n",
    "                endTime = final_survey_df['gaze'].iloc[i][-1]['time']\n",
    "                startTime = final_survey_df['gaze'].iloc[i][0]['time']\n",
    "\n",
    "                endTime = pd.to_datetime(endTime / 1000, unit='s')\n",
    "                startTime = pd.to_datetime(startTime / 1000, unit='s')\n",
    "                \n",
    "                duration = (endTime - startTime).total_seconds()\n",
    "            else:\n",
    "                duration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert start and end times to a meaningful duration and drop the start/end time columns<br>and the windowDimensions\n",
    "\n",
    "*Note: Currently the duration column is being dropped later, but it's here in case we want to incorporate this later*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_survey_df['startTime'] = final_survey_df['startTime'] / 1000\n",
    "final_survey_df['endTime'] = final_survey_df['endTime'] / 1000\n",
    "final_survey_df['startTime'] = pd.to_datetime(final_survey_df['startTime'], unit='s')\n",
    "final_survey_df['endTime'] = pd.to_datetime(final_survey_df['endTime'], unit='s')\n",
    "final_survey_df['duration'] = final_survey_df['endTime'] - final_survey_df['startTime']\n",
    "final_survey_df['duration'] = final_survey_df['duration'].dt.total_seconds()\n",
    "final_survey_df = final_survey_df.drop(columns=['startTime', 'endTime', 'windowDimensions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the users df with the survey df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = final_survey_df.merge(final_users_df, on='userId', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a key to the gaze dictionaries for if a hazard was present at that specific timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(merged_df.shape[0]):\n",
    "    if len(merged_df['gaze'][i]) == 0:\n",
    "        continue\n",
    "\n",
    "    min_time = min([gaze_point['time'] for gaze_point in merged_df['gaze'][i]])\n",
    "\n",
    "    for j in range(len(merged_df['gaze'][i])):\n",
    "        if merged_df['hazardDetected'][i] == False or len(merged_df['spacebarTimestamps'][i]) == 0:\n",
    "            merged_df['gaze'][i][j]['hazard'] = False\n",
    "        else:\n",
    "            k = 1\n",
    "            while k < len(merged_df['spacebarTimestamps'][i]):\n",
    "                time = merged_df['gaze'][i][j]['time']\n",
    "                time_during_hazard = time > merged_df['spacebarTimestamps'][i][k-1] and time < merged_df['spacebarTimestamps'][i][k]\n",
    "                if merged_df['hazardDetected'][i] == True and time_during_hazard:\n",
    "                    merged_df['gaze'][i][j]['hazard'] = True\n",
    "                else:\n",
    "                    merged_df['gaze'][i][j]['hazard'] = True\n",
    "                k += 2\n",
    "        \n",
    "        merged_df['gaze'][i][j]['time'] = (merged_df['gaze'][i][j]['time'] - min_time) / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the entire dataframe to have one row per timestamp with a value of if it's hazardous or not which will be the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>videoId</th>\n",
       "      <th>hazardDetected</th>\n",
       "      <th>noDetectionReason</th>\n",
       "      <th>detectionConfidence</th>\n",
       "      <th>hazardSeverity</th>\n",
       "      <th>attentionFactors</th>\n",
       "      <th>spacebarTimestamps</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>...</th>\n",
       "      <th>licenseAge</th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>visuallyImpaired</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>time</th>\n",
       "      <th>_id</th>\n",
       "      <th>hazard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jonahmulcrone@gmail.com</td>\n",
       "      <td>video219</td>\n",
       "      <td>False</td>\n",
       "      <td>noHazards</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1470</td>\n",
       "      <td>797</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>24.0</td>\n",
       "      <td>White</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "      <td>500.496763</td>\n",
       "      <td>499.953378</td>\n",
       "      <td>0.000</td>\n",
       "      <td>679564a790793f4f61cbbbd4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jonahmulcrone@gmail.com</td>\n",
       "      <td>video219</td>\n",
       "      <td>False</td>\n",
       "      <td>noHazards</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1470</td>\n",
       "      <td>797</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>24.0</td>\n",
       "      <td>White</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "      <td>500.898921</td>\n",
       "      <td>497.460984</td>\n",
       "      <td>0.197</td>\n",
       "      <td>679564a790793f4f61cbbbd5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jonahmulcrone@gmail.com</td>\n",
       "      <td>video219</td>\n",
       "      <td>False</td>\n",
       "      <td>noHazards</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1470</td>\n",
       "      <td>797</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>24.0</td>\n",
       "      <td>White</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "      <td>514.789690</td>\n",
       "      <td>496.588843</td>\n",
       "      <td>0.396</td>\n",
       "      <td>679564a790793f4f61cbbbd6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jonahmulcrone@gmail.com</td>\n",
       "      <td>video219</td>\n",
       "      <td>False</td>\n",
       "      <td>noHazards</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1470</td>\n",
       "      <td>797</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>24.0</td>\n",
       "      <td>White</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "      <td>553.986331</td>\n",
       "      <td>478.562368</td>\n",
       "      <td>0.595</td>\n",
       "      <td>679564a790793f4f61cbbbd7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonahmulcrone@gmail.com</td>\n",
       "      <td>video219</td>\n",
       "      <td>False</td>\n",
       "      <td>noHazards</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1470</td>\n",
       "      <td>797</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>24.0</td>\n",
       "      <td>White</td>\n",
       "      <td>male</td>\n",
       "      <td>True</td>\n",
       "      <td>606.061360</td>\n",
       "      <td>461.811118</td>\n",
       "      <td>0.794</td>\n",
       "      <td>679564a790793f4f61cbbbd8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40873</th>\n",
       "      <td>andersonlennox381@outlook.com</td>\n",
       "      <td>video401</td>\n",
       "      <td>False</td>\n",
       "      <td>noHazards</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1512</td>\n",
       "      <td>860</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>890.081049</td>\n",
       "      <td>120.326971</td>\n",
       "      <td>13.774</td>\n",
       "      <td>67a3ca8590793f4f61cc7022</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40874</th>\n",
       "      <td>andersonlennox381@outlook.com</td>\n",
       "      <td>video401</td>\n",
       "      <td>False</td>\n",
       "      <td>noHazards</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1512</td>\n",
       "      <td>860</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>1022.138557</td>\n",
       "      <td>145.055342</td>\n",
       "      <td>14.076</td>\n",
       "      <td>67a3ca8590793f4f61cc7023</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40875</th>\n",
       "      <td>andersonlennox381@outlook.com</td>\n",
       "      <td>video401</td>\n",
       "      <td>False</td>\n",
       "      <td>noHazards</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1512</td>\n",
       "      <td>860</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>888.994377</td>\n",
       "      <td>276.047783</td>\n",
       "      <td>14.376</td>\n",
       "      <td>67a3ca8590793f4f61cc7024</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40876</th>\n",
       "      <td>andersonlennox381@outlook.com</td>\n",
       "      <td>video401</td>\n",
       "      <td>False</td>\n",
       "      <td>noHazards</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1512</td>\n",
       "      <td>860</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>748.477336</td>\n",
       "      <td>311.445999</td>\n",
       "      <td>14.678</td>\n",
       "      <td>67a3ca8590793f4f61cc7025</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40877</th>\n",
       "      <td>andersonlennox381@outlook.com</td>\n",
       "      <td>video401</td>\n",
       "      <td>False</td>\n",
       "      <td>noHazards</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1512</td>\n",
       "      <td>860</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>703.261176</td>\n",
       "      <td>353.549660</td>\n",
       "      <td>14.977</td>\n",
       "      <td>67a3ca8590793f4f61cc7026</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40878 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              userId   videoId  hazardDetected  \\\n",
       "0            jonahmulcrone@gmail.com  video219           False   \n",
       "1            jonahmulcrone@gmail.com  video219           False   \n",
       "2            jonahmulcrone@gmail.com  video219           False   \n",
       "3            jonahmulcrone@gmail.com  video219           False   \n",
       "4            jonahmulcrone@gmail.com  video219           False   \n",
       "...                              ...       ...             ...   \n",
       "40873  andersonlennox381@outlook.com  video401           False   \n",
       "40874  andersonlennox381@outlook.com  video401           False   \n",
       "40875  andersonlennox381@outlook.com  video401           False   \n",
       "40876  andersonlennox381@outlook.com  video401           False   \n",
       "40877  andersonlennox381@outlook.com  video401           False   \n",
       "\n",
       "      noDetectionReason  detectionConfidence  hazardSeverity attentionFactors  \\\n",
       "0             noHazards                    5               0               []   \n",
       "1             noHazards                    5               0               []   \n",
       "2             noHazards                    5               0               []   \n",
       "3             noHazards                    5               0               []   \n",
       "4             noHazards                    5               0               []   \n",
       "...                 ...                  ...             ...              ...   \n",
       "40873         noHazards                    5               0               []   \n",
       "40874         noHazards                    5               0               []   \n",
       "40875         noHazards                    5               0               []   \n",
       "40876         noHazards                    5               0               []   \n",
       "40877         noHazards                    5               0               []   \n",
       "\n",
       "      spacebarTimestamps  width  height  ... licenseAge   age  \\\n",
       "0                     []   1470     797  ...         17  24.0   \n",
       "1                     []   1470     797  ...         17  24.0   \n",
       "2                     []   1470     797  ...         17  24.0   \n",
       "3                     []   1470     797  ...         17  24.0   \n",
       "4                     []   1470     797  ...         17  24.0   \n",
       "...                  ...    ...     ...  ...        ...   ...   \n",
       "40873                 []   1512     860  ...         18  23.0   \n",
       "40874                 []   1512     860  ...         18  23.0   \n",
       "40875                 []   1512     860  ...         18  23.0   \n",
       "40876                 []   1512     860  ...         18  23.0   \n",
       "40877                 []   1512     860  ...         18  23.0   \n",
       "\n",
       "                       ethnicity gender  visuallyImpaired            x  \\\n",
       "0                          White   male              True   500.496763   \n",
       "1                          White   male              True   500.898921   \n",
       "2                          White   male              True   514.789690   \n",
       "3                          White   male              True   553.986331   \n",
       "4                          White   male              True   606.061360   \n",
       "...                          ...    ...               ...          ...   \n",
       "40873  Black or African American   male             False   890.081049   \n",
       "40874  Black or African American   male             False  1022.138557   \n",
       "40875  Black or African American   male             False   888.994377   \n",
       "40876  Black or African American   male             False   748.477336   \n",
       "40877  Black or African American   male             False   703.261176   \n",
       "\n",
       "                y    time                       _id  hazard  \n",
       "0      499.953378   0.000  679564a790793f4f61cbbbd4   False  \n",
       "1      497.460984   0.197  679564a790793f4f61cbbbd5   False  \n",
       "2      496.588843   0.396  679564a790793f4f61cbbbd6   False  \n",
       "3      478.562368   0.595  679564a790793f4f61cbbbd7   False  \n",
       "4      461.811118   0.794  679564a790793f4f61cbbbd8   False  \n",
       "...           ...     ...                       ...     ...  \n",
       "40873  120.326971  13.774  67a3ca8590793f4f61cc7022   False  \n",
       "40874  145.055342  14.076  67a3ca8590793f4f61cc7023   False  \n",
       "40875  276.047783  14.376  67a3ca8590793f4f61cc7024   False  \n",
       "40876  311.445999  14.678  67a3ca8590793f4f61cc7025   False  \n",
       "40877  353.549660  14.977  67a3ca8590793f4f61cc7026   False  \n",
       "\n",
       "[40878 rows x 23 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.explode('gaze', ignore_index=True)\n",
    "normalized = pd.json_normalize(merged_df['gaze'])\n",
    "merged_df = merged_df.drop(columns=['gaze']).join(normalized)\n",
    "merged_df.drop(columns=['duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save The File (This will be the baseline before more EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df.to_csv('cleaned_survey_user_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode the necessary columns\n",
    "\n",
    "*Note: Can also create embeddings or other encoding methods if too many cols*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['userId', 'videoId', 'hazardDetected', 'detectionConfidence',\n",
      "       'hazardSeverity', 'width', 'height', 'duration', 'licenseAge', 'age',\n",
      "       'visuallyImpaired', 'x', 'y', 'time', 'hazard',\n",
      "       'attentionFactors_construction', 'attentionFactors_environment',\n",
      "       'attentionFactors_motion', 'attentionFactors_other',\n",
      "       'attentionFactors_pedestrian', 'attentionFactors_proximity',\n",
      "       'attentionFactors_velocity', 'noDetectionReason_nohazards',\n",
      "       'noDetectionReason_subtlehazards', 'noDetectionReason_uncertain',\n",
      "       'country_ar', 'country_fr', 'country_tn', 'country_us',\n",
      "       'state_california', 'state_florida', 'state_massachusetts',\n",
      "       'state_north carolina', 'state_oregon', 'state_south carolina',\n",
      "       'state_washington', 'city_boca raton', 'city_boston',\n",
      "       'city_chapel hill', 'city_charlotte', 'city_coconut creek',\n",
      "       'city_delray beach', 'city_durham', 'city_los angeles', 'city_miami',\n",
      "       'city_olympia', 'city_puyallup', 'city_raleigh', 'city_san diego',\n",
      "       'city_seattle', 'city_tega cay', 'city_west linn',\n",
      "       'city_woodland hills', 'ethnicity_asian',\n",
      "       'ethnicity_black or african american', 'ethnicity_hispanic or latino',\n",
      "       'ethnicity_middle eastern or north african', 'ethnicity_multiracial',\n",
      "       'ethnicity_prefer not to say', 'ethnicity_white', 'gender_female',\n",
      "       'gender_male', 'gender_prefer-not-to-say'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "attention_factors_exploded = merged_df.explode('attentionFactors')\n",
    "attn_factor_cols = pd.get_dummies(attention_factors_exploded['attentionFactors'], prefix='attentionFactors')\n",
    "merged_df = merged_df.drop(columns=['attentionFactors', 'spacebarTimestamps', '_id']).join(attn_factor_cols.groupby(level=0).max())\n",
    "\n",
    "columns_to_clean = ['noDetectionReason', 'country', 'state', 'city', 'ethnicity', 'gender']\n",
    "\n",
    "for col in columns_to_clean:\n",
    "    merged_df[col] = merged_df[col].replace('', pd.NA).fillna('ignore')\n",
    "    if merged_df[col].dtype == 'object':\n",
    "        merged_df[col] = merged_df[col].str.lower().replace('', pd.NA).fillna('ignore')\n",
    "\n",
    "merged_df['city'] = merged_df['city'].replace({'boca': 'boca raton'})\n",
    "\n",
    "encoded_columns = pd.get_dummies(merged_df, columns=columns_to_clean, prefix=columns_to_clean)\n",
    "merged_df = encoded_columns\n",
    "merged_df = merged_df.drop(merged_df.filter(like='_ignore').columns, axis=1)\n",
    "\n",
    "print(merged_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Rows with Missing Data (Because Ahmed's Bitchass didn't fill out the survey right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.dropna()\n",
    "#merged_df.to_csv('final_user_survey_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row indexes with NaN values in userId: 0.0\n",
      "Row indexes with NaN values in videoId: 0.0\n",
      "Row indexes with NaN values in hazardDetected: 0.0\n",
      "Row indexes with NaN values in detectionConfidence: 0.0\n",
      "Row indexes with NaN values in hazardSeverity: 0.0\n",
      "Row indexes with NaN values in width: 0.0\n",
      "Row indexes with NaN values in height: 0.0\n",
      "Row indexes with NaN values in duration: 0.0\n",
      "Row indexes with NaN values in licenseAge: 0.0\n",
      "Row indexes with NaN values in age: 0.0\n",
      "Row indexes with NaN values in visuallyImpaired: 0.0\n",
      "Row indexes with NaN values in x: 0.0\n",
      "Row indexes with NaN values in y: 0.0\n",
      "Row indexes with NaN values in time: 0.0\n",
      "Row indexes with NaN values in hazard: 0.0\n",
      "Row indexes with NaN values in attentionFactors_construction: 0.0\n",
      "Row indexes with NaN values in attentionFactors_environment: 0.0\n",
      "Row indexes with NaN values in attentionFactors_motion: 0.0\n",
      "Row indexes with NaN values in attentionFactors_other: 0.0\n",
      "Row indexes with NaN values in attentionFactors_pedestrian: 0.0\n",
      "Row indexes with NaN values in attentionFactors_proximity: 0.0\n",
      "Row indexes with NaN values in attentionFactors_velocity: 0.0\n",
      "Row indexes with NaN values in noDetectionReason_nohazards: 0.0\n",
      "Row indexes with NaN values in noDetectionReason_subtlehazards: 0.0\n",
      "Row indexes with NaN values in noDetectionReason_uncertain: 0.0\n",
      "Row indexes with NaN values in country_ar: 0.0\n",
      "Row indexes with NaN values in country_fr: 0.0\n",
      "Row indexes with NaN values in country_tn: 0.0\n",
      "Row indexes with NaN values in country_us: 0.0\n",
      "Row indexes with NaN values in state_california: 0.0\n",
      "Row indexes with NaN values in state_florida: 0.0\n",
      "Row indexes with NaN values in state_massachusetts: 0.0\n",
      "Row indexes with NaN values in state_north carolina: 0.0\n",
      "Row indexes with NaN values in state_oregon: 0.0\n",
      "Row indexes with NaN values in state_south carolina: 0.0\n",
      "Row indexes with NaN values in state_washington: 0.0\n",
      "Row indexes with NaN values in city_boca raton: 0.0\n",
      "Row indexes with NaN values in city_boston: 0.0\n",
      "Row indexes with NaN values in city_chapel hill: 0.0\n",
      "Row indexes with NaN values in city_charlotte: 0.0\n",
      "Row indexes with NaN values in city_coconut creek: 0.0\n",
      "Row indexes with NaN values in city_delray beach: 0.0\n",
      "Row indexes with NaN values in city_durham: 0.0\n",
      "Row indexes with NaN values in city_los angeles: 0.0\n",
      "Row indexes with NaN values in city_miami: 0.0\n",
      "Row indexes with NaN values in city_olympia: 0.0\n",
      "Row indexes with NaN values in city_puyallup: 0.0\n",
      "Row indexes with NaN values in city_raleigh: 0.0\n",
      "Row indexes with NaN values in city_san diego: 0.0\n",
      "Row indexes with NaN values in city_seattle: 0.0\n",
      "Row indexes with NaN values in city_tega cay: 0.0\n",
      "Row indexes with NaN values in city_west linn: 0.0\n",
      "Row indexes with NaN values in city_woodland hills: 0.0\n",
      "Row indexes with NaN values in ethnicity_asian: 0.0\n",
      "Row indexes with NaN values in ethnicity_black or african american: 0.0\n",
      "Row indexes with NaN values in ethnicity_hispanic or latino: 0.0\n",
      "Row indexes with NaN values in ethnicity_middle eastern or north african: 0.0\n",
      "Row indexes with NaN values in ethnicity_multiracial: 0.0\n",
      "Row indexes with NaN values in ethnicity_prefer not to say: 0.0\n",
      "Row indexes with NaN values in ethnicity_white: 0.0\n",
      "Row indexes with NaN values in gender_female: 0.0\n",
      "Row indexes with NaN values in gender_male: 0.0\n",
      "Row indexes with NaN values in gender_prefer-not-to-say: 0.0\n"
     ]
    }
   ],
   "source": [
    "for col in merged_df.columns:\n",
    "    nan_rows = merged_df[merged_df[col].isna()]\n",
    "    print(f\"Row indexes with NaN values in {col}: {len(nan_rows) / merged_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert video data to 0.5s splits and replace hazard binary data by majority vote per video per time bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = merged_df.copy()\n",
    "\n",
    "time_split=0.28\n",
    "\n",
    "df_copy['time_bin'] = (df_copy['time'] // time_split).astype(int)  # Create bins of 0.5 seconds\n",
    "\n",
    "user_grouped = (\n",
    "    df_copy.groupby(['userId', 'videoId', 'time_bin'])\n",
    "    .agg({\n",
    "        'x': 'mean',  # Average x position in the interval\n",
    "        'y': 'mean',  # Average y position in the interval\n",
    "        'hazard': 'any'  # True if any row in the interval is hazardous\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 2: Calculate majority hazard vote per videoId and time_bin\n",
    "majority_hazard = (\n",
    "    user_grouped.groupby(['videoId', 'time_bin'])['hazard']\n",
    "    .apply(lambda hazards: hazards.mean() > 0.5)  # True if majority voted True\n",
    "    .reset_index(name='majority_hazard')\n",
    ")\n",
    "\n",
    "# Step 3: Merge back to individual user data, replacing the original hazard value\n",
    "final_grouped = pd.merge(user_grouped, majority_hazard, on=['videoId', 'time_bin'])\n",
    "final_grouped['hazard'] = final_grouped['majority_hazard']  # Replace with majority decision\n",
    "final_grouped = final_grouped.drop(columns=['majority_hazard'])\n",
    "\n",
    "# Step 4: Create time column representing the start of the interval\n",
    "final_grouped['time'] = final_grouped['time_bin'] * time_split\n",
    "\n",
    "# Drop the 'time_bin' column if not needed\n",
    "final_grouped = final_grouped.drop('time_bin', axis=1)\n",
    "final_grouped.to_csv('binned_video_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 0.7569016218185425\n",
      "Current Loss: 0.7755458652973175\n",
      "Current Loss: 0.7832516630490621\n",
      "Current Loss: 0.7302255928516388\n",
      "Current Loss: 0.726823377609253\n",
      "Current Loss: 0.7324932714303335\n",
      "Current Loss: 0.7592461960656303\n",
      "Current Loss: 0.7286547943949699\n",
      "Epoch 1, Loss: 0.7287\n",
      "Current Loss: 0.062136292457580566\n",
      "Current Loss: 0.20388907194137573\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Modify the final layer for binary classification\u001b[39;00m\n\u001b[1;32m     47\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, device)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/EDA/training.py:157\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, loader, device)\u001b[0m\n\u001b[1;32m    154\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    155\u001b[0m j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clips, labels \u001b[38;5;129;01min\u001b[39;00m loader:  \u001b[38;5;66;03m# Use DataLoader properly for batches\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     clips \u001b[38;5;241m=\u001b[39m clips\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move to GPU if available\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/EDA/training.py:40\u001b[0m, in \u001b[0;36mVideoDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m videoId \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideoId\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     38\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 40\u001b[0m video_tensor, fps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_video_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43muserId\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideoId\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m start_time_in_seconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, time \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_splits)\n\u001b[1;32m     43\u001b[0m start_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(start_time_in_seconds \u001b[38;5;241m*\u001b[39m fps)\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/EDA/training.py:32\u001b[0m, in \u001b[0;36mVideoDataset.load_video_frames\u001b[0;34m(self, userId, videoId)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_video_frames\u001b[39m(\u001b[38;5;28mself\u001b[39m, userId, videoId):\n\u001b[1;32m     31\u001b[0m     video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/training_videos/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muserId\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideoId\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 32\u001b[0m     frames, fps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_video_if_exist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m frames, fps\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/EDA/training.py:66\u001b[0m, in \u001b[0;36mVideoDataset.get_video_if_exist\u001b[0;34m(self, output_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m     frames_with_positions\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# If not reducing the video resolution you can use the line below\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#frames = [torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0 for frame in frames]\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m frames \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(Image\u001b[38;5;241m.\u001b[39mfromarray(cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB))) \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames_with_positions]\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Pad if fewer frames\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes_per_clip:\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/EDA/training.py:66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m     frames_with_positions\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# If not reducing the video resolution you can use the line below\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#frames = [torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0 for frame in frames]\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m frames \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames_with_positions]\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Pad if fewer frames\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes_per_clip:\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/venv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/venv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/venv/lib/python3.9/site-packages/torchvision/transforms/functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/venv/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AIPI_549/ComputerVision/Human-Alignment-Hazardous-Driving-Detection/venv/lib/python3.9/site-packages/PIL/Image.py:2356\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2344\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2345\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2346\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[1;32m   2347\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2348\u001b[0m         )\n\u001b[1;32m   2349\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2350\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2351\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2352\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2353\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2354\u001b[0m         )\n\u001b[0;32m-> 2356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "from training import VideoDataset, evaluate, training_loop\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def stratify_split_data(X, y, train_size=20):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=train_size, random_state=42)\n",
    "    for subset_idx, _ in sss.split(X, y):\n",
    "        stratified_df = training_df.iloc[subset_idx]\n",
    "        stratified_labels = labels.iloc[subset_idx]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        stratified_df, stratified_labels, test_size=0.2, stratify=stratified_labels, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('./binned_video_data.csv')  # Replace with your actual file\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "time_split = df.iloc[1]['time']\n",
    "train_size = 50\n",
    "\n",
    "training_df = df.drop(columns=['hazard'])\n",
    "labels = df['hazard']\n",
    "\n",
    "X_train, X_test, y_train, y_test = stratify_split_data(training_df, labels, train_size)\n",
    "\n",
    "train_dataset = VideoDataset(X_train, y_train, time_splits=time_split, frames_per_clip=540, clip_size=36*time_split)\n",
    "test_dataset = VideoDataset(X_test, y_test, time_splits=time_split, frames_per_clip=540, clip_size=36*time_split)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "# Load the pre-trained I3D model\n",
    "model = r3d_18(weights=R3D_18_Weights.KINETICS400_V1)\n",
    "\n",
    "# Freeze weights\n",
    "'''\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "'''\n",
    "\n",
    "# Modify the final layer for binary classification\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "\n",
    "training_loop(model, train_loader, device)\n",
    "\n",
    "accuracy = evaluate(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2001796676149124\n"
     ]
    }
   ],
   "source": [
    "print(df[df['hazard'] == True].shape[0] / df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save All Videos Per User Per Video with Eye tracking overlayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Iterate through all unique userId and videoId combinations\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (selected_user, selected_video) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmerged_df\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideoId\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mvalues:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Filter the data for the current userId and videoId\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     filtered_data \u001b[38;5;241m=\u001b[39m merged_df[(merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m selected_user) \u001b[38;5;241m&\u001b[39m (merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideoId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m selected_video)]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Skip if no data for the current combination\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Iterate through all unique userId and videoId combinations\n",
    "for (selected_user, selected_video) in merged_df[['userId', 'videoId']].drop_duplicates().values:\n",
    "    # Filter the data for the current userId and videoId\n",
    "    filtered_data = merged_df[(merged_df['userId'] == selected_user) & (merged_df['videoId'] == selected_video)].reset_index(drop=True)\n",
    "    \n",
    "    # Skip if no data for the current combination\n",
    "    if filtered_data.empty:\n",
    "        continue\n",
    "    \n",
    "    # Interpolation function\n",
    "    def interpolate_positions(times, xs, ys, fps):\n",
    "        new_times = np.arange(times[0], times[-1], 1 / fps)\n",
    "        new_xs = np.interp(new_times, times, xs)\n",
    "        new_ys = np.interp(new_times, times, ys)\n",
    "        return new_times, new_xs, new_ys\n",
    "\n",
    "    # Extract time, x, y columns for interpolation\n",
    "    times = filtered_data['time'].values\n",
    "    xs = filtered_data['x'].values\n",
    "    ys = filtered_data['y'].values\n",
    "\n",
    "    # Load the video\n",
    "    video_path = f'./data/driving_videos/{selected_video}.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if video is loaded correctly\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Warning: Could not open video {video_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Output video setup\n",
    "    output_file = f'./data/training_videos/{selected_user}_{selected_video}.mp4'\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Interpolate the positions\n",
    "    interpolated_times, interpolated_xs, interpolated_ys = interpolate_positions(times, xs, ys, fps)\n",
    "\n",
    "    # Initialize current frame index\n",
    "    frame_num = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or frame_num >= len(interpolated_times):\n",
    "            break\n",
    "\n",
    "        # Get the interpolated position for the current frame\n",
    "        x, y = int(interpolated_xs[frame_num]), int(interpolated_ys[frame_num])\n",
    "\n",
    "        # Draw a circle at the interpolated position\n",
    "        cv2.circle(frame, (x, y), 10, (0, 255, 0), -1)  # Green circle for normal\n",
    "\n",
    "        # Write the current frame to the output\n",
    "        out.write(frame)\n",
    "        frame_num += 1\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Example Vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "data = merged_df.copy()\n",
    "\n",
    "selected_user = 'Avimarzini123@gmail.com'  # Replace with userId you want\n",
    "selected_video = 'video423'  # Replace with videoId you want\n",
    "\n",
    "filtered_data = data[(data['userId'] == selected_user) & (data['videoId'] == selected_video)].reset_index(drop=True)\n",
    "\n",
    "# Interpolation function between (x, y) points\n",
    "def interpolate_positions(times, xs, ys, fps):\n",
    "    new_times = np.arange(times[0], times[-1], 1 / fps)\n",
    "    new_xs = np.interp(new_times, times, xs)\n",
    "    new_ys = np.interp(new_times, times, ys)\n",
    "    return new_times, new_xs, new_ys\n",
    "\n",
    "# Extract time, x, y columns for interpolation\n",
    "times = filtered_data['time'].values\n",
    "xs = filtered_data['x'].values\n",
    "ys = filtered_data['y'].values\n",
    "\n",
    "# Load the video\n",
    "video_path = './data/driving_videos/' + selected_video + '.mp4'  # Replace with the video path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(fps)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Output video setup\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output_video.mp4', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Interpolate the positions\n",
    "interpolated_times, interpolated_xs, interpolated_ys = interpolate_positions(times, xs, ys, fps)\n",
    "\n",
    "# Initialize current frame index\n",
    "frame_num = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or frame_num >= len(interpolated_times):\n",
    "        break\n",
    "\n",
    "    # Get the interpolated position for the current frame\n",
    "    x, y = int(interpolated_xs[frame_num]), int(interpolated_ys[frame_num])\n",
    "\n",
    "    # Draw a circle at the interpolated position\n",
    "    cv2.circle(frame, (x, y), 10, (0, 255, 0), -1)  # Green circle for normal\n",
    "\n",
    "    # Write the current frame to the output\n",
    "    out.write(frame)\n",
    "    frame_num += 1\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
