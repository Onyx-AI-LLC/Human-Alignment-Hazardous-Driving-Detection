{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d193666-b5c3-4399-8122-9b486e7b5af6",
   "metadata": {},
   "source": [
    "# Eye-Tracking Data Processing and Visualization Pipeline\n",
    "\n",
    "Scripts were developed with the help of claude 3.5 Sonnet \n",
    "\n",
    "This collection of scripts processes eye-tracking data from driving simulations:\n",
    "\n",
    "Data Processing:\n",
    "- Normalizes raw gaze coordinates from various screen sizes to standard 1280x960 resolution\n",
    "- Handles letterboxing/pillarboxing offsets across different displays\n",
    "- Filters out low-quality data where users looked at screen edges >50% of time\n",
    "- Separates data into preprocessedAndNormalized.csv and badgazedata.csv\n",
    "\n",
    "Visualization Features:\n",
    "- Creates individual videos showing user gaze patterns with smooth animations\n",
    "- Uses cubic interpolation for fluid eye movement transitions \n",
    "- Displays gaze as glowing hot pink dot overlaid on driving footage\n",
    "- Color-codes points by original screen size with detailed legend\n",
    "- Shows normalized vs original coordinates for verification\n",
    "\n",
    "Video Players:\n",
    "- General player showing all driving videos with gaze overlay\n",
    "- Hazard-specific player showing only videos with detected hazards\n",
    "- Displays metadata like hazard severity, video ID, and timestamps\n",
    "- Includes playback controls (pause/play/quit)\n",
    "\n",
    "File Generation:\n",
    "- Creates separate MP4 files named {user_email}_{video_id}.mp4\n",
    "- Preserves original coordinate information and metadata\n",
    "- Provides detailed processing summaries and progress tracking\n",
    "- Includes error handling and data validation\n",
    "\n",
    "Usage:\n",
    "- Press 'p' to pause/unpause video playback\n",
    "- Press 'q' to quit video playback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e608dd-8f78-45d2-ba9d-f8f93f63efd5",
   "metadata": {},
   "source": [
    "# Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e4de354f-f13b-4d17-9618-b61f9381a2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dataloader in /opt/anaconda3/lib/python3.12/site-packages (2.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /opt/anaconda3/lib/python3.12/site-packages (from scipy) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install dataloader\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ae4d6-38e2-4807-8235-008314fbd2cf",
   "metadata": {},
   "source": [
    "# Driving Video Gaze Visualization Player \n",
    "Randomly selects and plays driving videos while overlaying real-time eye-tracking data that shows where users were looking during the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a713db3b-2825-4540-84c0-ad4c0cd47e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class DrivingVideoPlayer:\n",
    "    def __init__(self, video_folder_path, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the video player with paths to video folder and gaze data CSV\n",
    "        \n",
    "        Args:\n",
    "            video_folder_path (str): Path to folder containing driving videos\n",
    "            csv_path (str): Path to CSV file containing gaze data\n",
    "        \"\"\"\n",
    "        self.video_folder = Path(video_folder_path)\n",
    "        self.gaze_data = pd.read_csv(csv_path)\n",
    "        self.available_videos = list(self.video_folder.glob('*.mp4'))  # Adjust extension if needed\n",
    "        \n",
    "    def get_random_video(self):\n",
    "        \"\"\"Select a random video from the folder\"\"\"\n",
    "        if not self.available_videos:\n",
    "            raise FileNotFoundError(\"No videos found in specified folder\")\n",
    "        return random.choice(self.available_videos)\n",
    "    \n",
    "    def get_gaze_data_for_video(self, video_id):\n",
    "        \"\"\"\n",
    "        Get all gaze data points for a specific video\n",
    "        \n",
    "        Args:\n",
    "            video_id (str): ID of the video to get gaze data for\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Filtered gaze data for the specified video\n",
    "        \"\"\"\n",
    "        return self.gaze_data[self.gaze_data['videoId'] == video_id].sort_values('time')\n",
    "    \n",
    "    def draw_gaze_points(self, frame, gaze_points, current_time, time_window=0.5):\n",
    "        \"\"\"\n",
    "        Draw gaze points on the frame\n",
    "        \n",
    "        Args:\n",
    "            frame: Current video frame\n",
    "            gaze_points (pd.DataFrame): Gaze data for the current video\n",
    "            current_time (float): Current time in the video\n",
    "            time_window (float): Time window to show gaze points (in seconds)\n",
    "        \"\"\"\n",
    "        # Filter gaze points within the time window\n",
    "        current_points = gaze_points[\n",
    "            (gaze_points['time'] >= current_time - time_window) & \n",
    "            (gaze_points['time'] <= current_time)\n",
    "        ]\n",
    "        \n",
    "        # Draw each gaze point\n",
    "        for _, point in current_points.iterrows():\n",
    "            x, y = int(point['x']), int(point['y'])\n",
    "            # Draw point with user ID\n",
    "            cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n",
    "            cv2.putText(frame, f\"User: {point['userId']}\", \n",
    "                       (x + 10, y - 10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       0.5, (0, 255, 0), 1)\n",
    "    \n",
    "    def play_random_video(self):\n",
    "        \"\"\"Play a random video with overlaid gaze data\"\"\"\n",
    "        # Get random video\n",
    "        video_path = self.get_random_video()\n",
    "        video_id = video_path.stem  # Assumes filename matches video ID in CSV\n",
    "        \n",
    "        # Get gaze data for this video\n",
    "        video_gaze_data = self.get_gaze_data_for_video(video_id)\n",
    "        \n",
    "        # Open video\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video: {video_path}\")\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            current_time = frame_count / fps\n",
    "            \n",
    "            # Draw gaze points for current time\n",
    "            self.draw_gaze_points(frame, video_gaze_data, current_time)\n",
    "            \n",
    "            # Add time indicator\n",
    "            cv2.putText(frame, f\"Time: {current_time:.2f}s\", \n",
    "                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       1, (255, 255, 255), 2)\n",
    "            \n",
    "            # Show frame\n",
    "            cv2.imshow('Driving Video with Gaze Data', frame)\n",
    "            \n",
    "            # Handle keyboard input\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):  \n",
    "                break\n",
    "            elif key == ord('p'):  \n",
    "                cv2.waitKey(0)\n",
    "            \n",
    "            frame_count += 1\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    \n",
    "    video_folder = \"/Users/lennoxanderson/Documents/machineLearning/data/TeslaRawDrivingFootage/SplitData/1-4BatchSplits\"  # Replace with your video folder path\n",
    "    csv_path = \"/Users/lennoxanderson/Documents/Research/Human-Alignment-Hazardous-Driving-Detection/final_user_survey_data.csv\"  # Replace with your CSV file path\n",
    "    \n",
    "    try:\n",
    "        player = DrivingVideoPlayer(video_folder, csv_path)\n",
    "        player.play_random_video()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08f86f-0aa6-4be6-8549-ffa8aeb9a71d",
   "metadata": {},
   "source": [
    "# Hazardous Driving Video Analyzer \n",
    "Selectively plays driving videos containing hazards while displaying both the hazard severity and real-time eye-tracking data of users who viewed these potentially dangerous situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f08aa4fa-5c92-4b78-b9e5-4d38f4b9e114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 130 videos with hazards\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class DrivingVideoPlayer:\n",
    "    def __init__(self, video_folder_path, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the video player with paths to video folder and gaze data CSV\n",
    "        \n",
    "        Args:\n",
    "            video_folder_path (str): Path to folder containing driving videos\n",
    "            csv_path (str): Path to CSV file containing gaze data\n",
    "        \"\"\"\n",
    "        self.video_folder = Path(video_folder_path)\n",
    "        self.gaze_data = pd.read_csv(csv_path)\n",
    "        # Get unique videos that had hazards detected\n",
    "        hazard_videos = self.gaze_data[self.gaze_data['hazardDetected'] == True]['videoId'].unique()\n",
    "        # Filter available videos to only those with hazards\n",
    "        self.available_videos = [\n",
    "            video for video in self.video_folder.glob('*.mp4')\n",
    "            if video.stem in hazard_videos\n",
    "        ]\n",
    "        print(f\"Found {len(self.available_videos)} videos with hazards\")\n",
    "        \n",
    "    def get_random_hazard_video(self):\n",
    "        \"\"\"Select a random video from the folder that contains a hazard\"\"\"\n",
    "        if not self.available_videos:\n",
    "            raise FileNotFoundError(\"No videos with hazards found in specified folder\")\n",
    "        return random.choice(self.available_videos)\n",
    "    \n",
    "    def get_gaze_data_for_video(self, video_id):\n",
    "        \"\"\"\n",
    "        Get all gaze data points for a specific video\n",
    "        \n",
    "        Args:\n",
    "            video_id (str): ID of the video to get gaze data for\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Filtered gaze data for the specified video\n",
    "        \"\"\"\n",
    "        return self.gaze_data[self.gaze_data['videoId'] == video_id].sort_values('time')\n",
    "    \n",
    "    def draw_gaze_points(self, frame, gaze_points, current_time, time_window=0.5):\n",
    "        \"\"\"\n",
    "        Draw gaze points on the frame\n",
    "        \n",
    "        Args:\n",
    "            frame: Current video frame\n",
    "            gaze_points (pd.DataFrame): Gaze data for the current video\n",
    "            current_time (float): Current time in the video\n",
    "            time_window (float): Time window to show gaze points (in seconds)\n",
    "        \"\"\"\n",
    "        # Filter gaze points within the time window\n",
    "        current_points = gaze_points[\n",
    "            (gaze_points['time'] >= current_time - time_window) & \n",
    "            (gaze_points['time'] <= current_time)\n",
    "        ]\n",
    "        \n",
    "        # Draw each gaze point\n",
    "        for _, point in current_points.iterrows():\n",
    "            x, y = int(point['x']), int(point['y'])\n",
    "            # Draw point with user ID\n",
    "            cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n",
    "            cv2.putText(frame, f\"User: {point['userId']}\", \n",
    "                       (x + 10, y - 10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       0.5, (0, 255, 0), 1)\n",
    "    \n",
    "    def play_random_hazard_video(self):\n",
    "        \"\"\"Play a random video containing a hazard with overlaid gaze data\"\"\"\n",
    "        # Get random video with hazard\n",
    "        video_path = self.get_random_hazard_video()\n",
    "        video_id = video_path.stem\n",
    "        \n",
    "        # Get video metadata\n",
    "        video_metadata = self.gaze_data[self.gaze_data['videoId'] == video_id].iloc[0]\n",
    "        hazard_severity = video_metadata['hazardSeverity']\n",
    "        \n",
    "        # Get gaze data for this video\n",
    "        video_gaze_data = self.get_gaze_data_for_video(video_id)\n",
    "        \n",
    "        # Open video\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video: {video_path}\")\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            current_time = frame_count / fps\n",
    "            \n",
    "            # Draw gaze points for current time\n",
    "            self.draw_gaze_points(frame, video_gaze_data, current_time)\n",
    "            \n",
    "            # Add time indicator and hazard info\n",
    "            cv2.putText(frame, f\"Time: {current_time:.2f}s\", \n",
    "                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       1, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f\"Video ID: {video_id}\", \n",
    "                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       1, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f\"Hazard Severity: {hazard_severity}\", \n",
    "                       (10, 90), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       1, (255, 255, 255), 2)\n",
    "            \n",
    "            # Show frame\n",
    "            cv2.imshow('Driving Video with Hazard and Gaze Data', frame)\n",
    "            \n",
    "            # Handle keyboard input\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):  # Press 'q' to quit\n",
    "                break\n",
    "            elif key == ord('p'):  # Press 'p' to pause/unpause\n",
    "                cv2.waitKey(0)\n",
    "            \n",
    "            frame_count += 1\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    video_folder = \"/Users/lennoxanderson/Documents/machineLearning/data/TeslaRawDrivingFootage/SplitData/1-4BatchSplits\"\n",
    "    csv_path = \"/Users/lennoxanderson/Documents/Research/Human-Alignment-Hazardous-Driving-Detection/final_user_survey_data.csv\"\n",
    "    \n",
    "    try:\n",
    "        player = DrivingVideoPlayer(video_folder, csv_path)\n",
    "        player.play_random_hazard_video()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2303b6-7683-4eb0-87a3-e01d109a7c6d",
   "metadata": {},
   "source": [
    "# Normalized Hazard Video Gaze Visualizer\n",
    "Plays hazardous driving videos while displaying color-coded eye-tracking data that has been normalized across different screen sizes to the original video resolution (1280x960), making gaze patterns directly comparable regardless of the viewer's original screen dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "fc17c59e-fc3e-489d-9e7f-d5d0737d3630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized gaze coordinates to video resolution (1280x960)\n",
      "Found 130 videos with hazards\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class DrivingVideoPlayer:\n",
    "    def __init__(self, video_folder_path, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize the video player with paths to video folder and gaze data CSV\n",
    "        \"\"\"\n",
    "        self.video_folder = Path(video_folder_path)\n",
    "        self.gaze_data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Video dimensions (both original and target are the same)\n",
    "        self.video_width = 1280\n",
    "        self.video_height = 960\n",
    "        \n",
    "        # Normalize all gaze coordinates\n",
    "        self.normalize_gaze_coordinates()\n",
    "        \n",
    "        # Get unique videos that had hazards detected\n",
    "        hazard_videos = self.gaze_data[self.gaze_data['hazardDetected'] == True]['videoId'].unique()\n",
    "        self.available_videos = [\n",
    "            video for video in self.video_folder.glob('*.mp4')\n",
    "            if video.stem in hazard_videos\n",
    "        ]\n",
    "        print(f\"Found {len(self.available_videos)} videos with hazards\")\n",
    "    \n",
    "    def calculate_video_display_size(self, screen_width, screen_height):\n",
    "        \"\"\"\n",
    "        Calculate how the video would be displayed on a given screen size\n",
    "        while maintaining aspect ratio\n",
    "        \"\"\"\n",
    "        video_aspect = self.video_width / self.video_height\n",
    "        screen_aspect = screen_width / screen_height\n",
    "        \n",
    "        if screen_aspect > video_aspect:\n",
    "            # Height limited\n",
    "            display_height = screen_height\n",
    "            display_width = display_height * video_aspect\n",
    "        else:\n",
    "            # Width limited\n",
    "            display_width = screen_width\n",
    "            display_height = display_width / video_aspect\n",
    "            \n",
    "        return display_width, display_height\n",
    "    \n",
    "    def normalize_gaze_coordinates(self):\n",
    "        \"\"\"\n",
    "        Normalize all gaze coordinates to the original video resolution (1280x960)\n",
    "        accounting for how the video was displayed on different screen sizes\n",
    "        \"\"\"\n",
    "        normalized_coordinates = []\n",
    "        \n",
    "        for _, row in self.gaze_data.iterrows():\n",
    "            # Calculate how video was displayed on user's screen\n",
    "            display_width, display_height = self.calculate_video_display_size(\n",
    "                row['width'], row['height']\n",
    "            )\n",
    "            \n",
    "            # Calculate the black bars (letterbox/pillarbox) offset\n",
    "            x_offset = (row['width'] - display_width) / 2\n",
    "            y_offset = (row['height'] - display_height) / 2\n",
    "            \n",
    "            # Remove the offset from the gaze coordinates\n",
    "            adjusted_x = row['x'] - x_offset\n",
    "            adjusted_y = row['y'] - y_offset\n",
    "            \n",
    "            # Convert from display coordinates to video coordinates\n",
    "            video_x = (adjusted_x / display_width) * self.video_width\n",
    "            video_y = (adjusted_y / display_height) * self.video_height\n",
    "            \n",
    "            # Store original values for reference\n",
    "            row['original_x'] = row['x']\n",
    "            row['original_y'] = row['y']\n",
    "            \n",
    "            # Update coordinates with normalized values\n",
    "            row['x'] = np.clip(video_x, 0, self.video_width)\n",
    "            row['y'] = np.clip(video_y, 0, self.video_height)\n",
    "            \n",
    "            normalized_coordinates.append(row)\n",
    "        \n",
    "        self.gaze_data = pd.DataFrame(normalized_coordinates)\n",
    "        print(\"Normalized gaze coordinates to video resolution (1280x960)\")\n",
    "\n",
    "    def get_random_hazard_video(self):\n",
    "        \"\"\"Select a random video from the folder that contains a hazard\"\"\"\n",
    "        if not self.available_videos:\n",
    "            raise FileNotFoundError(\"No videos with hazards found in specified folder\")\n",
    "        return random.choice(self.available_videos)\n",
    "    \n",
    "    def get_gaze_data_for_video(self, video_id):\n",
    "        \"\"\"Get all gaze data points for a specific video\"\"\"\n",
    "        return self.gaze_data[self.gaze_data['videoId'] == video_id].sort_values('time')\n",
    "    \n",
    "    def draw_gaze_points(self, frame, gaze_points, current_time, time_window=0.5):\n",
    "        \"\"\"Draw gaze points on the frame\"\"\"\n",
    "        # Filter gaze points within the time window\n",
    "        current_points = gaze_points[\n",
    "            (gaze_points['time'] >= current_time - time_window) & \n",
    "            (gaze_points['time'] <= current_time)\n",
    "        ]\n",
    "        \n",
    "        # Draw each gaze point\n",
    "        for _, point in current_points.iterrows():\n",
    "            x, y = int(point['x']), int(point['y'])\n",
    "            \n",
    "            # Color based on original screen size\n",
    "            original_size = f\"{int(point['width'])}x{int(point['height'])}\"\n",
    "            color_hash = hash(original_size) % 255\n",
    "            point_color = (color_hash, 255, (color_hash + 125) % 255)\n",
    "            \n",
    "            # Draw point\n",
    "            cv2.circle(frame, (x, y), 5, point_color, -1)\n",
    "            \n",
    "            # Draw information about the point\n",
    "            cv2.putText(frame, f\"User: {point['userId']}\", \n",
    "                       (x + 10, y - 25), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       0.5, point_color, 1)\n",
    "            cv2.putText(frame, f\"Screen: {original_size}\", \n",
    "                       (x + 10, y - 10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       0.5, point_color, 1)\n",
    "            \n",
    "            # Draw original coordinates for verification\n",
    "            if 'original_x' in point:\n",
    "                cv2.putText(frame, \n",
    "                           f\"Orig: ({int(point['original_x'])},{int(point['original_y'])})\", \n",
    "                           (x + 10, y + 5), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                           0.5, point_color, 1)\n",
    "\n",
    "    def play_random_hazard_video(self):\n",
    "        \"\"\"Play a random video containing a hazard with overlaid gaze data\"\"\"\n",
    "        video_path = self.get_random_hazard_video()\n",
    "        video_id = video_path.stem\n",
    "        \n",
    "        # Get video metadata\n",
    "        video_metadata = self.gaze_data[self.gaze_data['videoId'] == video_id].iloc[0]\n",
    "        hazard_severity = video_metadata['hazardSeverity']\n",
    "        \n",
    "        # Get gaze data for this video\n",
    "        video_gaze_data = self.get_gaze_data_for_video(video_id)\n",
    "        \n",
    "        # Get unique screen sizes for legend\n",
    "        screen_sizes = video_gaze_data[['width', 'height']].drop_duplicates()\n",
    "        \n",
    "        # Open video\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video: {video_path}\")\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            current_time = frame_count / fps\n",
    "            \n",
    "            # Draw gaze points for current time\n",
    "            self.draw_gaze_points(frame, video_gaze_data, current_time)\n",
    "            \n",
    "            # Add information overlay\n",
    "            cv2.putText(frame, f\"Time: {current_time:.2f}s\", \n",
    "                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       1, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f\"Video ID: {video_id}\", \n",
    "                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       1, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f\"Hazard Severity: {hazard_severity}\", \n",
    "                       (10, 90), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       1, (255, 255, 255), 2)\n",
    "            \n",
    "            # Add screen size legend\n",
    "            y_offset = 120\n",
    "            cv2.putText(frame, \"Original Screen Sizes:\", \n",
    "                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       0.7, (255, 255, 255), 2)\n",
    "            for _, size in screen_sizes.iterrows():\n",
    "                y_offset += 25\n",
    "                size_str = f\"{int(size['width'])}x{int(size['height'])}\"\n",
    "                color_hash = hash(size_str) % 255\n",
    "                color = (color_hash, 255, (color_hash + 125) % 255)\n",
    "                cv2.putText(frame, size_str, \n",
    "                           (30, y_offset), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                           0.6, color, 2)\n",
    "            \n",
    "            # Show frame\n",
    "            cv2.imshow('Driving Video with Normalized Gaze Data', frame)\n",
    "            \n",
    "            # Handle keyboard input\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('p'):\n",
    "                cv2.waitKey(0)\n",
    "            \n",
    "            frame_count += 1\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    video_folder = \"/Users/lennoxanderson/Documents/machineLearning/data/TeslaRawDrivingFootage/SplitData/1-4BatchSplits\"\n",
    "    csv_path = \"/Users/lennoxanderson/Documents/Research/Human-Alignment-Hazardous-Driving-Detection/final_user_survey_data.csv\"\n",
    "    \n",
    "    try:\n",
    "        player = DrivingVideoPlayer(video_folder, csv_path)\n",
    "        player.play_random_hazard_video()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67c436-ddde-46a8-9f1b-d30c7e3a7b9c",
   "metadata": {},
   "source": [
    "# Gaze Data Screen Size Normalizer\n",
    "Reads raw eye-tracking data collected from various screen sizes and normalizes all gaze coordinates to match the original video dimensions (1280x960), accounting for letterboxing and pillarboxing while preserving a detailed record of the transformations applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e70c3383-7bd0-46db-a111-165acebbab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from /Users/lennoxanderson/Documents/Research/Human-Alignment-Hazardous-Driving-Detection/final_user_survey_data.csv\n",
      "Normalizing gaze coordinates...\n",
      "Processing row 0/40653\n",
      "Processing row 1000/40653\n",
      "Processing row 2000/40653\n",
      "Processing row 3000/40653\n",
      "Processing row 4000/40653\n",
      "Processing row 5000/40653\n",
      "Processing row 6000/40653\n",
      "Processing row 7000/40653\n",
      "Processing row 8000/40653\n",
      "Processing row 9000/40653\n",
      "Processing row 10000/40653\n",
      "Processing row 11000/40653\n",
      "Processing row 12000/40653\n",
      "Processing row 13000/40653\n",
      "Processing row 14000/40653\n",
      "Processing row 15000/40653\n",
      "Processing row 16000/40653\n",
      "Processing row 17000/40653\n",
      "Processing row 18000/40653\n",
      "Processing row 19000/40653\n",
      "Processing row 20000/40653\n",
      "Processing row 21000/40653\n",
      "Processing row 22000/40653\n",
      "Processing row 23000/40653\n",
      "Processing row 24000/40653\n",
      "Processing row 25000/40653\n",
      "Processing row 26000/40653\n",
      "Processing row 27000/40653\n",
      "Processing row 28000/40653\n",
      "Processing row 29000/40653\n",
      "Processing row 30000/40653\n",
      "Processing row 31000/40653\n",
      "Processing row 32000/40653\n",
      "Processing row 33000/40653\n",
      "Processing row 34000/40653\n",
      "Processing row 35000/40653\n",
      "Processing row 36000/40653\n",
      "Processing row 37000/40653\n",
      "Processing row 38000/40653\n",
      "Processing row 39000/40653\n",
      "Processing row 40000/40653\n",
      "Saving normalized data to normalized_gaze_data.csv\n",
      "\n",
      "Normalization Summary:\n",
      "Total rows processed: 40653\n",
      "Original screen sizes: 29\n",
      "All rows normalized to: 1280x960\n",
      "\n",
      "Columns modified:\n",
      "- x, y: Normalized to video space coordinates\n",
      "- width, height: Set to video dimensions\n",
      "\n",
      "New columns added:\n",
      "- original_x\n",
      "- original_y\n",
      "- original_width\n",
      "- original_height\n",
      "- display_width\n",
      "- display_height\n",
      "- x_offset\n",
      "- y_offset\n",
      "- normalized_to_width\n",
      "- normalized_to_height\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def calculate_video_display_size(screen_width, screen_height, video_width=1280, video_height=960):\n",
    "    \"\"\"\n",
    "    Calculate how the video would be displayed on a given screen size\n",
    "    while maintaining aspect ratio\n",
    "    \"\"\"\n",
    "    video_aspect = video_width / video_height\n",
    "    screen_aspect = screen_width / screen_height\n",
    "    \n",
    "    if screen_aspect > video_aspect:\n",
    "        # Height limited\n",
    "        display_height = screen_height\n",
    "        display_width = display_height * video_aspect\n",
    "    else:\n",
    "        # Width limited\n",
    "        display_width = screen_width\n",
    "        display_height = display_width / video_aspect\n",
    "        \n",
    "    # Calculate letterbox/pillarbox offsets\n",
    "    x_offset = (screen_width - display_width) / 2\n",
    "    y_offset = (screen_height - display_height) / 2\n",
    "    \n",
    "    return display_width, display_height, x_offset, y_offset\n",
    "\n",
    "def normalize_gaze_data(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Read the input CSV, normalize gaze coordinates, and save to a new CSV\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from {input_csv}\")\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Original video dimensions\n",
    "    VIDEO_WIDTH = 1280\n",
    "    VIDEO_HEIGHT = 960\n",
    "    \n",
    "    # Lists to store normalized coordinates and additional info\n",
    "    normalized_data = []\n",
    "    \n",
    "    print(\"Normalizing gaze coordinates...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx % 1000 == 0:  # Progress indicator\n",
    "            print(f\"Processing row {idx}/{len(df)}\")\n",
    "            \n",
    "        # Calculate display dimensions and offsets\n",
    "        display_width, display_height, x_offset, y_offset = calculate_video_display_size(\n",
    "            row['width'], row['height']\n",
    "        )\n",
    "        \n",
    "        # Remove the offset from the gaze coordinates\n",
    "        adjusted_x = row['x'] - x_offset\n",
    "        adjusted_y = row['y'] - y_offset\n",
    "        \n",
    "        # Convert from display coordinates to video coordinates\n",
    "        video_x = (adjusted_x / display_width) * VIDEO_WIDTH\n",
    "        video_y = (adjusted_y / display_height) * VIDEO_HEIGHT\n",
    "        \n",
    "        # Clip coordinates to video boundaries\n",
    "        normalized_x = np.clip(video_x, 0, VIDEO_WIDTH)\n",
    "        normalized_y = np.clip(video_y, 0, VIDEO_HEIGHT)\n",
    "        \n",
    "        # Create new row with additional normalization info\n",
    "        new_row = row.copy()\n",
    "        \n",
    "        # Store original coordinates and dimensions\n",
    "        new_row['original_x'] = row['x']\n",
    "        new_row['original_y'] = row['y']\n",
    "        new_row['original_width'] = row['width']\n",
    "        new_row['original_height'] = row['height']\n",
    "        \n",
    "        # Store display calculations\n",
    "        new_row['display_width'] = display_width\n",
    "        new_row['display_height'] = display_height\n",
    "        new_row['x_offset'] = x_offset\n",
    "        new_row['y_offset'] = y_offset\n",
    "        \n",
    "        # Update main coordinates and dimensions to video space\n",
    "        new_row['x'] = normalized_x\n",
    "        new_row['y'] = normalized_y\n",
    "        new_row['width'] = VIDEO_WIDTH\n",
    "        new_row['height'] = VIDEO_HEIGHT\n",
    "        \n",
    "        # Add normalization metadata\n",
    "        new_row['normalized_to_width'] = VIDEO_WIDTH\n",
    "        new_row['normalized_to_height'] = VIDEO_HEIGHT\n",
    "        \n",
    "        normalized_data.append(new_row)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    normalized_df = pd.DataFrame(normalized_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    print(f\"Saving normalized data to {output_csv}\")\n",
    "    normalized_df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nNormalization Summary:\")\n",
    "    print(f\"Total rows processed: {len(normalized_df)}\")\n",
    "    print(f\"Original screen sizes: {len(normalized_df.groupby(['original_width', 'original_height']))}\")\n",
    "    print(f\"All rows normalized to: {VIDEO_WIDTH}x{VIDEO_HEIGHT}\")\n",
    "    print(\"\\nColumns modified:\")\n",
    "    print(\"- x, y: Normalized to video space coordinates\")\n",
    "    print(\"- width, height: Set to video dimensions\")\n",
    "    print(\"\\nNew columns added:\")\n",
    "    new_columns = ['original_x', 'original_y', 'original_width', 'original_height',\n",
    "                  'display_width', 'display_height', 'x_offset', 'y_offset',\n",
    "                  'normalized_to_width', 'normalized_to_height']\n",
    "    for col in new_columns:\n",
    "        print(f\"- {col}\")\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    input_csv = \"/Users/lennoxanderson/Documents/Research/Human-Alignment-Hazardous-Driving-Detection/final_user_survey_data.csv\"\n",
    "    output_csv = \"normalized_gaze_data.csv\"\n",
    "    \n",
    "    try:\n",
    "        normalize_gaze_data(input_csv, output_csv)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b217f-156b-4e19-9837-6e27f90612b5",
   "metadata": {},
   "source": [
    "# Smooth Gaze Path Video Generator\n",
    "Creates individualized videos for each user showing their eye movement patterns as a smoothly animated hot pink dot, using cubic interpolation to create fluid transitions between gaze points and generating separate video files for each user's viewing session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2518ba6a-baf9-4ba8-9840-4734605eb739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and normalizing gaze data...\n",
      "Found 20 unique users\n",
      "\n",
      "Creating sample video for user Avimarzini123@gmail.com\n",
      "Processing video video423 for user Avimarzini123@gmail.com\n",
      "Saved video to user_gaze_videos/Avimarzini123@gmail.com_video423.mp4\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you want to process all videos for all users? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing videos for user Avimarzini123@gmail.com\n",
      "Processing video video423 for user Avimarzini123@gmail.com\n",
      "Saved video to user_gaze_videos/Avimarzini123@gmail.com_video423.mp4\n",
      "\n",
      "Processing videos for user abdouliejaye7@gmail.com\n",
      "Processing video video227 for user abdouliejaye7@gmail.com\n",
      "Saved video to user_gaze_videos/abdouliejaye7@gmail.com_video227.mp4\n",
      "Processing video video383 for user abdouliejaye7@gmail.com\n",
      "Saved video to user_gaze_videos/abdouliejaye7@gmail.com_video383.mp4\n",
      "Processing video video8 for user abdouliejaye7@gmail.com\n",
      "Saved video to user_gaze_videos/abdouliejaye7@gmail.com_video8.mp4\n",
      "\n",
      "Processing videos for user amiterez93@gmail.com\n",
      "Processing video video129 for user amiterez93@gmail.com\n",
      "Saved video to user_gaze_videos/amiterez93@gmail.com_video129.mp4\n",
      "Processing video video483 for user amiterez93@gmail.com\n",
      "Saved video to user_gaze_videos/amiterez93@gmail.com_video483.mp4\n",
      "\n",
      "Processing videos for user andersonlennox381@outlook.com\n",
      "Processing video video442 for user andersonlennox381@outlook.com\n",
      "Saved video to user_gaze_videos/andersonlennox381@outlook.com_video442.mp4\n",
      "\n",
      "Processing videos for user andrew.rodov@gmail.com\n",
      "Processing video video149 for user andrew.rodov@gmail.com\n",
      "Saved video to user_gaze_videos/andrew.rodov@gmail.com_video149.mp4\n",
      "Processing video video23 for user andrew.rodov@gmail.com\n",
      "Saved video to user_gaze_videos/andrew.rodov@gmail.com_video23.mp4\n",
      "Processing video video35 for user andrew.rodov@gmail.com\n",
      "Saved video to user_gaze_videos/andrew.rodov@gmail.com_video35.mp4\n",
      "Processing video video509 for user andrew.rodov@gmail.com\n",
      "Saved video to user_gaze_videos/andrew.rodov@gmail.com_video509.mp4\n",
      "Processing video video52 for user andrew.rodov@gmail.com\n",
      "Saved video to user_gaze_videos/andrew.rodov@gmail.com_video52.mp4\n",
      "\n",
      "Processing videos for user charlesyeganeh@gmail.com\n",
      "Processing video video259 for user charlesyeganeh@gmail.com\n",
      "Saved video to user_gaze_videos/charlesyeganeh@gmail.com_video259.mp4\n",
      "Processing video video299 for user charlesyeganeh@gmail.com\n",
      "Saved video to user_gaze_videos/charlesyeganeh@gmail.com_video299.mp4\n",
      "Processing video video38 for user charlesyeganeh@gmail.com\n",
      "Saved video to user_gaze_videos/charlesyeganeh@gmail.com_video38.mp4\n",
      "Processing video video381 for user charlesyeganeh@gmail.com\n",
      "Saved video to user_gaze_videos/charlesyeganeh@gmail.com_video381.mp4\n",
      "Processing video video487 for user charlesyeganeh@gmail.com\n",
      "Saved video to user_gaze_videos/charlesyeganeh@gmail.com_video487.mp4\n",
      "Processing video video508 for user charlesyeganeh@gmail.com\n",
      "Saved video to user_gaze_videos/charlesyeganeh@gmail.com_video508.mp4\n",
      "Processing video video88 for user charlesyeganeh@gmail.com\n",
      "Saved video to user_gaze_videos/charlesyeganeh@gmail.com_video88.mp4\n",
      "\n",
      "Processing videos for user cph82@miami.edu\n",
      "Processing video video217 for user cph82@miami.edu\n",
      "Saved video to user_gaze_videos/cph82@miami.edu_video217.mp4\n",
      "Processing video video287 for user cph82@miami.edu\n",
      "Saved video to user_gaze_videos/cph82@miami.edu_video287.mp4\n",
      "Processing video video32 for user cph82@miami.edu\n",
      "Saved video to user_gaze_videos/cph82@miami.edu_video32.mp4\n",
      "\n",
      "Processing videos for user designappeal19@gmail.com\n",
      "Processing video video111 for user designappeal19@gmail.com\n",
      "Saved video to user_gaze_videos/designappeal19@gmail.com_video111.mp4\n",
      "Processing video video366 for user designappeal19@gmail.com\n",
      "Saved video to user_gaze_videos/designappeal19@gmail.com_video366.mp4\n",
      "\n",
      "Processing videos for user eitansays@gmail.com\n",
      "Processing video video104 for user eitansays@gmail.com\n",
      "Saved video to user_gaze_videos/eitansays@gmail.com_video104.mp4\n",
      "Processing video video144 for user eitansays@gmail.com\n",
      "Error: Expect x to not have duplicates\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "class UserGazeVideoCreator:\n",
    "    def __init__(self, video_folder_path, csv_path, output_folder_path):\n",
    "        \"\"\"\n",
    "        Initialize the video creator with paths\n",
    "        \"\"\"\n",
    "        self.video_folder = Path(video_folder_path)\n",
    "        self.output_folder = Path(output_folder_path)\n",
    "        self.output_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Hot pink color in BGR format\n",
    "        self.dot_color = (147, 20, 255)  # RGB(255, 20, 147) in BGR\n",
    "        \n",
    "        # Read and normalize the gaze data\n",
    "        print(\"Reading and normalizing gaze data...\")\n",
    "        self.gaze_data = self.read_normalized_gaze_data(csv_path)\n",
    "        \n",
    "        # Get unique users\n",
    "        self.users = self.gaze_data['userId'].unique()\n",
    "        print(f\"Found {len(self.users)} unique users\")\n",
    "\n",
    "    def read_normalized_gaze_data(self, csv_path):\n",
    "        \"\"\"Read and normalize the gaze data\"\"\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        VIDEO_WIDTH = 1280\n",
    "        VIDEO_HEIGHT = 960\n",
    "        \n",
    "        # Normalize coordinates if they haven't been normalized yet\n",
    "        if 'normalized_to_width' not in df.columns:\n",
    "            print(\"Normalizing coordinates...\")\n",
    "            for idx, row in df.iterrows():\n",
    "                # Calculate display dimensions\n",
    "                video_aspect = VIDEO_WIDTH / VIDEO_HEIGHT\n",
    "                screen_aspect = row['width'] / row['height']\n",
    "                \n",
    "                if screen_aspect > video_aspect:\n",
    "                    display_height = row['height']\n",
    "                    display_width = display_height * video_aspect\n",
    "                else:\n",
    "                    display_width = row['width']\n",
    "                    display_height = display_width / video_aspect\n",
    "                \n",
    "                # Calculate offsets\n",
    "                x_offset = (row['width'] - display_width) / 2\n",
    "                y_offset = (row['height'] - display_height) / 2\n",
    "                \n",
    "                # Normalize coordinates\n",
    "                df.at[idx, 'x'] = ((row['x'] - x_offset) / display_width) * VIDEO_WIDTH\n",
    "                df.at[idx, 'y'] = ((row['y'] - y_offset) / display_height) * VIDEO_HEIGHT\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def interpolate_gaze_points(self, video_gaze_data, fps):\n",
    "        \"\"\"\n",
    "        Create smooth interpolation between gaze points\n",
    "        \"\"\"\n",
    "        # Get original time points and coordinates\n",
    "        times = video_gaze_data['time'].values\n",
    "        x_coords = video_gaze_data['x'].values\n",
    "        y_coords = video_gaze_data['y'].values\n",
    "        \n",
    "        # Create interpolation functions\n",
    "        x_interp = interp1d(times, x_coords, kind='cubic', bounds_error=False, fill_value='extrapolate')\n",
    "        y_interp = interp1d(times, y_coords, kind='cubic', bounds_error=False, fill_value='extrapolate')\n",
    "        \n",
    "        # Create timestamps for every frame\n",
    "        frame_times = np.arange(times[0], times[-1], 1/fps)\n",
    "        \n",
    "        # Interpolate positions for every frame\n",
    "        x_smooth = x_interp(frame_times)\n",
    "        y_smooth = y_interp(frame_times)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'time': frame_times,\n",
    "            'x': x_smooth,\n",
    "            'y': y_smooth\n",
    "        })\n",
    "\n",
    "    def create_video_for_user(self, user_id, sample_mode=False):\n",
    "        \"\"\"\n",
    "        Create videos for a specific user with their gaze overlay\n",
    "        \n",
    "        Args:\n",
    "            user_id (str): User ID (email) to process\n",
    "            sample_mode (bool): If True, only process one video for this user\n",
    "        \"\"\"\n",
    "        # Get all videos for this user\n",
    "        user_data = self.gaze_data[self.gaze_data['userId'] == user_id]\n",
    "        unique_videos = user_data['videoId'].unique()\n",
    "        \n",
    "        if sample_mode:\n",
    "            unique_videos = unique_videos[:1]  # Only process first video in sample mode\n",
    "        \n",
    "        for video_id in unique_videos:\n",
    "            # Get gaze data for this video\n",
    "            video_gaze_data = user_data[user_data['videoId'] == video_id].sort_values('time')\n",
    "            \n",
    "            if len(video_gaze_data) < 4:  # Need at least 4 points for cubic interpolation\n",
    "                print(f\"Not enough gaze points for video {video_id}\")\n",
    "                continue\n",
    "                \n",
    "            # Find video file\n",
    "            video_path = next(self.video_folder.glob(f\"{video_id}.*\"))\n",
    "            if not video_path.exists():\n",
    "                print(f\"Video file not found for {video_id}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing video {video_id} for user {user_id}\")\n",
    "            \n",
    "            # Open video\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Could not open video: {video_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Get video properties\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            \n",
    "            # Create interpolated gaze points\n",
    "            smooth_gaze_data = self.interpolate_gaze_points(video_gaze_data, fps)\n",
    "            \n",
    "            # Create output video\n",
    "            output_filename = f\"{user_id}_{video_id}.mp4\"\n",
    "            output_path = self.output_folder / output_filename\n",
    "            \n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(str(output_path), fourcc, fps, (frame_width, frame_height))\n",
    "            \n",
    "            frame_count = 0\n",
    "            last_gaze_time = smooth_gaze_data['time'].max()\n",
    "            \n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                current_time = frame_count / fps\n",
    "                \n",
    "                # Stop if we've passed the last gaze point\n",
    "                if current_time > last_gaze_time:\n",
    "                    break\n",
    "                \n",
    "                # Get interpolated gaze point for current time\n",
    "                current_point = smooth_gaze_data[\n",
    "                    (smooth_gaze_data['time'] >= current_time) &\n",
    "                    (smooth_gaze_data['time'] < current_time + 1/fps)\n",
    "                ]\n",
    "                \n",
    "                if not current_point.empty:\n",
    "                    x = int(current_point.iloc[0]['x'])\n",
    "                    y = int(current_point.iloc[0]['y'])\n",
    "                    \n",
    "                    # Draw dot with anti-aliasing\n",
    "                    cv2.circle(frame, (x, y), 8, self.dot_color, -1, cv2.LINE_AA)\n",
    "                    \n",
    "                    # Add subtle glow effect\n",
    "                    cv2.circle(frame, (x, y), 12, self.dot_color, 2, cv2.LINE_AA)\n",
    "                \n",
    "                out.write(frame)\n",
    "                frame_count += 1\n",
    "            \n",
    "            cap.release()\n",
    "            out.release()\n",
    "            print(f\"Saved video to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    video_folder = \"/Users/lennoxanderson/Documents/machineLearning/data/TeslaRawDrivingFootage/SplitData/1-4BatchSplits\"\n",
    "    csv_path = \"/Users/lennoxanderson/Documents/Research/Human-Alignment-Hazardous-Driving-Detection/ETL/badgazedata.csv\"  # Use your normalized CSV\n",
    "    output_folder = \"user_gaze_videos\"\n",
    "    \n",
    "    try:\n",
    "        creator = UserGazeVideoCreator(video_folder, csv_path, output_folder)\n",
    "        \n",
    "        # First create a sample video for one user\n",
    "        sample_user = creator.users[0]\n",
    "        print(f\"\\nCreating sample video for user {sample_user}\")\n",
    "        creator.create_video_for_user(sample_user, sample_mode=True)\n",
    "        \n",
    "        # Ask if user wants to process all videos\n",
    "        response = input(\"\\nDo you want to process all videos for all users? (y/n): \")\n",
    "        if response.lower() == 'y':\n",
    "            for user_id in creator.users:\n",
    "                print(f\"\\nProcessing videos for user {user_id}\")\n",
    "                creator.create_video_for_user(user_id)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
